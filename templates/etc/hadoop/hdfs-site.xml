<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>{{data_replication_factor}}</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///var/local/hadoop/dfs/data</value>
    <description>If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices.</description>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///var/local/hadoop/dfs/name</value>
    <description>If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.</description>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file:///var/local/hadoop/dfs/namesecondary</value>
    <description>Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.</description>
  </property>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>{{namenode_host}}:50070</value>
    <description>The namenode http server address and port.</description>
  </property>
  <property>
    <name>dfs.namenode.https-address</name>
    <value>{{namenode_host}}:50071</value>
    <description>The namenode HTTPS server address and port.</description>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>{{secondarynamenode_host}}:50090</value>
    <description>The secondary namenode http server address and port.</description>
  </property>
  <property>
    <name>dfs.namenode.secondary.https-address</name>
    <value>{{secondarynamenode_host}}:50091</value>
    <description>The secondary namenode HTTPS server address and port.</description>
  </property>
  <property>
    <name>dfs.datanode.address</name>
    <value>{{datanode_host}}:50010</value>
    <description>The datanode server address and port for data transfer.</description>
  </property>
  <property>
    <name>dfs.datanode.http.address</name>
    <value>{{datanode_host}}:50075</value>
    <description>The datanode http server address and port.</description>
  </property>
  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>{{datanode_host}}:50020</value>
    <description>The datanode ipc server address and port.</description>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value>67108864</value>
    <description>The default block size for new files, in bytes.</description>
  </property>
  <property>
    <name>dfs.hosts</name>
    <value/>
    <description>Names a file that contains a list of hosts that are permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, all hosts are permitted.</description>
  </property>
  <property>
    <name>dfs.hosts.exclude</name>
    <value/>
    <description>Names a file that contains a list of hosts that are not permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, no hosts are excluded.</description>
  </property>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
    <description>The number of server threads for the namenode. More NameNode server threads to handle RPCs from large number of DataNodes.</description>
  </property>
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>hadoop</value>
    <description>The name of the group of super-users.</description>
  </property>
  <property>
    <name>dfs.permissions.enabled</name>
    <value>true</value>
    <description>If "true", enable permission checking in HDFS. If "false", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories.</description>
  </property>
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
    <description>Enable WebHDFS (REST API) in Namenodes and Datanodes.</description>
  </property>
</configuration>
